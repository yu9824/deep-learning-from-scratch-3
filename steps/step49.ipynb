{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baef0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if '__file__' in globals():\n",
    "    import os, sys\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "import dezero.functions as F\n",
    "from dezero import optimizers\n",
    "from dezero.models import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652fe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca11ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a918b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Create minibatch\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
